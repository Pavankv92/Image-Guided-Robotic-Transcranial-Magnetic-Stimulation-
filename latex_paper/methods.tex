% !TEX root = paper.tex

\section{Methods}

% \lipsum[1]
There are three essential problems to be solved. First we solve the robot kinematics. We then implement algorithms for depth camera and hand to eye calibration. Lastly, we implement a procedure for tracking the head and then combine these parts into a single working system.

\input{Kinematics}

\subsection{Camera Calibration [SE]}

For the head tracking, a Microsoft Kinect V2 is used. This sensor uses a normal RGB camera combined with an infrared based depth camera. In order to provide accurate tracking, the cameras have to be calibrated. The camera intrinsic calibration is needed to undistort the images. The camera matrix of the RGB camera is also used for the 3D reconstruction in hand-eye calibration. 
As we are using both depth and RGB informations for the tracking, the extrinsic calibration, that means the mapping of the RGB camera to the depth camera, is needed. 
% #TODO do we need source here?

The camera calibration is done using the computer vision library OpenCV \cite{opencv_library}. In order to access the Kinect from ROS, the package IAI Kinect2 \cite{iai_kinect2} is used. The rectification and undistortion process using the calibration parameters is also handled by this software.

\paragraph{Intrinsic Calibration}

The used method for the camera intrinsic calibration is based on the algorithm from Zhang \cite{Zhang}. 
To apply the calibration method, several different image of a know calibration pattern, a chessboard in this case, must be recorded. 
The inputs for that algorithm are then combinations of image points and object points. The image points are the 2D locations of the chessboard corners in each image. The object points represent the calibration pattern and are in 3D in real world space. For simplification, the world coordinate system is assumed to be at the chessboard plane so that the $z$ coordinate is always zero. 

The locations of the image points are first determined by classical image processing 
procedures and then refined in an iterative manner to achieve sub pixel accuracy.
% #TODO more details? We just use an OpenCV function

The depth camera has a much lower resolution than the RGB camera. The images or the calibration of that camera are therefore scaled up using cubic interpolation. Without that the detection of chess bord corners is only possible for small distances.

We use 40 images for the calibration. The OpenCV implementation of the calibration algorithm returns the root mean square (RMS) re-projection error. For the RGB camera we achieved an error of $0.110$ pixels and for the depth camera an error of $0.136$ pixels. 


% \lipsum[1] % Dummy text

\paragraph{Extrinsic Calibration}
As described above, our setup requires a RGB camera to depth camera calibration. During this procedure, the transformation between both cameras is estimated by comparing poses of the calibration pattern relative to the RGB camera and relative to the depth camera. We use the same 80 images as used for the intrinsic calibrations, as they were already taken synchronously. 
 

% \lipsum[1] % Dummy text

\subsection{Hand-Eye Calibration [PV]}
 \begin{figure}[ht]
	\centering
 	\includegraphics[width=\linewidth]{Handeye}
 	\caption{Hand eye setup}
 	\source{\cite{WinNT} (modified)}
 	\label{Handeyecalibrationsetup}
 \end{figure}


Figure \ref{Handeyecalibrationsetup} depicts the robot and camera set up required for hand-eye calibration. There are four transformations, namely end  to base  ${}^{Base}T_{EE}$, chessboard to end effector  ${}^{EE}T_{CB}$, chessboard to camera  ${}^{Cam}T_{CB}$ and camera to base  ${}^{Base}T_{Cam}$. Defining new variables  ${}^{Base}T_{EE}$ as M,  ${}^{EE}T_{CB}$ as X,  ${}^{Base}T_{Cam}$ as Y and  ${}^{Cam}T_{CB}$ as N we can write the transformation equation as 
\begin{equation}
    MX = YN
    \label{eqn:HandEye}
\end{equation}
% #TODO Why do you name them in one way and then define other names in the next sentence? - to be consistance with the cited paper.

M is obtained through forward kinematics, N is obtained by calculating the pose of the calibration pattern from the camera image. 
As the equation system \ref{eqn:HandEye} is underdetermined, we need several different poses to be able to get a solution. We defined over 60 different poses, always making sure that the chessboard is completely visible in the camera image. The calibration process is automated. The robot drives to each of the defined positions where an image was taken using the RGB camera. The pose of the chessboard in camera coordinates is determined for each position by using the camera extrinsic calibration.


Hand eye calibration aims to solve the two unknowns in (\ref{eqn:HandEye}) i.e the chessboard position with respect to the robot's end effector X and the camera position with respect to the robot's base Y. The equation is transformed to the form Aw = b as presented in the equations (4), (5) and (6) of \cite{ernst2012non}. However, $N_i$’s in the equation (6) of \cite{ernst2012non} is replaced by inverse of $N_i$’s. Non trivial elements of matrices X and Y were computed by solving Aw = b in a least square method.


\subsection{Head Tracking [SP]}

In order to facilitate the tracking of head movement, we need to obtain three-dimensional data of the head and process it further. The Kinect depth camera provides head data for geometric alignment. Iterative Closest Point (ICP) algorithm is widely used to align the three dimensional models when an initial estimate of the relative pose is known.

Inputs for ICP registration are two point clouds and an initial transformation that aligns the source point cloud with the target point cloud approximately. We evaluated two variants of ICP registration named point-to-point ICP and point-to-plane ICP. 
After preliminary trials, we decided to use the point-to-plane method, which is based on an algorithm by Yang Chen $\cite{chen1992object}$ to register our point clouds since it achieves quicker convergence.

In general, ICP iterates over 2 steps:
\begin{enumerate}[leftmargin=*]
    \item Find correspondence set $S=\{p,q\}$ from target point cloud $p$ and source point cloud $q$ which is transformed with current transformation matrix $T$.
    \item Update the transformation $T$ by minimizing the objective function $E(T)$ \eqref{eqn:objective_fn} which is defined over correspondence set $S$.
\end{enumerate}
\begin{equation}
E(T)=\sum_{p,q\subset{S}}{((p-Tq)\cdot n_p)^2}
\label{eqn:objective_fn}
\end{equation}
$n_p$ is the normal of point $p$ \cite{rusinkiewicz2001efficient} in objective function $E(T)$. The point-to-plane approach needs estimation of plane normals. The algorithm finds  adjacent points and calculates the principle axis of adjacent points using covariance analysis. Internally, this algorithm uses jacobian matrices and computes residuals of the point-to-plane ICP objective.

We employ the open source library Open3D \cite{Open3D}, which provides point cloud processing and an implementation of point-to-plane ICP.
 
% \lipsum[1] % Dummy text

\subsection{Implementation [LK]}
We combine the head tracking with matrices from calibrations and inverse kinematics to create a tracking system as shown in fig. \ref{Implementation}.

\begin{figure}[ht]
	\centering
 	\includegraphics[width=\linewidth]{implementation2}
 	\caption{Transformations used for tracking}
 	\label{Implementation}
 \end{figure}

Raw point cloud data is preprocessed to remove outlier points which are not part of the face geometry by using thresholds and color filters. In figure \ref{Implementation} two head point clouds are shown, where the blue one is the initial one saved at the beginning.

The matrix from the hand eye calibration ${}^{Base}T_{Cam}$ is multiplied and combined with the initial head offset from the camera to calculate a transformation ${}^{Base}T_{Init}$, which is fixed during tracking. Also an inital offset ${}^{Init}T_{EE}$ of the endeffector from the head is calculated using ${}^{Base}T_{Init}$ and forward kinematics. 

When tracking a new pose, Open3D point-to-plane ICP is used to obtain a transformation matrix between the initial head position and the current one called ${}^{Init}T_{New}$. 
\begin{equation}
{}^{Base}T_{EEnew}={}^{Base}T_{Init} {}^{Init}T_{New} {}^{Init}T_{EE}
\label{eqn:implementation}
\end{equation}
These matrices can be multiplied as seen in \eqref{eqn:implementation} to obtain the matrix ${}^{Base}T_{EEnew}$ which is translated into joint angles with the use of inverse kinematics. 

% \lipsum[1] % Dummy text
